# -*- coding: utf-8 -*-
import nltk
import random

modalVerbs = ['can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will', 'would']

def feature(sent):
    lSent = sent.split()
    features = {}
    features["firstword"] = lSent[0].lower()
    if len(lSent) > 2:
        features["secondword"] = lSent[1].lower()
    features["firstletter"] = sent[0].lower()
    features["lastletter"] = sent[-1].lower()
    for word in modalVerbs:
        features["count(%s)" % word] = sent.lower().count(word)
        features["has(%s)" % word] = (word in lSent)
    return features

sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')    
    
normSents = sent_tokenizer.tokenize(file('normsList.txt', 'r').read())
noNormSents = sent_tokenizer.tokenize(file('noNormList.txt', 'r').read())
    
sentences = ([(feature(sent), 'norm') for sent in normSents] + [(feature(sent), 'noNorm') for sent in noNormSents[:2981]])

limit = int(len(sentences) * .8)

random.shuffle(sentences)

trainSet, testSet = sentences[:limit], sentences[limit:]

classifier = nltk.NaiveBayesClassifier.train(trainSet)

print nltk.classify.accuracy(classifier, testSet)

print classifier.show_most_informative_features(5)

print classifier.classify(feature("Charlie must obey the candy and go to candy mountain."))

print classifier.classify(feature('Today is a good day, I should try to see you!'))
