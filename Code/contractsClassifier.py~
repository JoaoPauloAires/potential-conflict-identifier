"""
    This algorithm receives all contracts extracted from Xibin's base and then classify their sentences.
    After, it evaluates the f-measure for the task.
"""

from __future__ import division
from dictionaryNorms import *
from normClassifier import *
import os
import hashlib
import nltk
import threading
from multiprocessing.pool import ThreadPool
import time

truePositive, trueNegative, falsePositive, falseNegative = 0, 0, 0, 0
output = open('data/output.txt', 'w')
dictionary = dic()   

def verify(sent, result):
    global truePositive
    global trueNegative
    global falsePositive
    global falseNegative
    global dictionary
        
    key = hashlib.md5(sent).digest()
    
    if dictionary.has_key(key):
        if result == 'norm':
            truePositive = truePositive + 1
        else:
            falseNegative = falseNegative + 1
    else:
        if result == 'norm':
            falsePositive = falsePositive + 1
        else:
            trueNegative = trueNegative + 1

def evaluate():
    global truePositive
    global trueNegative
    global falsePositive
    global falseNegative
    precision = truePositive/(truePositive+falsePositive)
    recall = truePositive/(truePositive+falseNegative)
    fMeasure = (2*precision*recall)/(precision + recall)
    output.write("\nTrue positive: "+ str(truePositive)+ "\nTrue Negative: "+ str(trueNegative)+ "\nFalse Positive: "+ str(falsePositive)+ "\nFalse Negative: "+ str(falseNegative))
    output.write("\nPrecision: "+str(precision)+"\nRecall: "+str(recall)+"\nF-Measure: "+str(fMeasure)+"\n")
    output.close()

def cleanSent(sent):
    lSent = sent.split()
    for s in lSent:
        if '--' in s:
            lSent.pop(lSent.index(s))    
    if len(lSent) > 1:
        if lSent[1] in ['and', 'or', 'will', 'of', 'and/or']:
            return ' '.join(lSent)
        elif '(' in lSent[0] or '.' in lSent[0]:
            return ' '.join(lSent[1:])
    return ' '.join(lSent)

def classifies(s, classifier):
    if classifier(feature(s)) == 'norm':
        s = cleanSent(s)
        verify(s, 'norm')
    else:
        verify(s, 'noNorm')

        
def classify():
    directory = '/home/lsa/Dropbox/PUCRS/Dissertation/Corpus/xibinCorpus/noHTML/'
    listDir = os.listdir(directory)
    threadList = []
    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')
    classifier = start()
    for dir in listDir:
        print dir
        listFiles = os.listdir(directory+dir)
        for file in listFiles:
            print "\t "+file+" "+str(listFiles.index(file)+1)+"/"+str(len(listFiles))
            f = open(directory+dir+"/"+file, 'r').read().split()[1:-1]
            f = ' '.join(f)
            sents = sent_tokenizer.tokenize(f)
            for s in sents:
                t = threading.Thread(target=classifies,args=(s,classifier))
                t.start()
                for tL in threadList:
                    tL.join()

    evaluate()                            	

classify()    
