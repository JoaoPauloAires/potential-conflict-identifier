"""
    This algorithm receives each sentence from the norm set defined by Xibin.
    Then, it classifier it either as norm or not.    
"""

#from normClassifier import *
import nltk
import os
import hashlib

sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
directory = '/home/lsa/Dropbox/PUCRS/Dissertation/ContractMiner/ContractMiner/ContractMiner/data/norm/'
listFiles = os.listdir(directory)
#output = open('data/extractedNorms.txt', 'w')
#classifier = start()

def extractVowels(sent):
    count = 0
    length = len(sent)
    listS = sent.split()
    for i in sent:
        if i.lower() in ['a', 'e', 'i', 'o', 'u']:
            count = count + 1
    key = str(count) + "+" + str(length) + "+" + str(length-count) + "+" + listS[0] + "+" + listS[-1]
    return key
                
def main():
    totalCounter = 0
    normCounter = 0
#    a = {}
    for file in listFiles:
        listSent = open(directory+file, 'r').readlines()
#        listSent = sent_tokenizer.tokenize(normFile)
#        for sent in listSent:
#            a[hashlib.md5(sent.rstrip('\r\n')).digest()] = sent.rstrip('\r\n')
#            totalCounter = totalCounter + 1
#            if classifier(feature(sent[:-2])) == 'norm':
#                normCounter = normCounter + 1
#                output.write(str(totalCounter)+" - "+sent[:-2]+'\n\n')
#    output.close()
    print "Total: ", totalCounter
    print "Norms: ", normCounter
    print "Hit Percentage: ", (normCounter*100)/totalCounter
    return listSent
#    return a
    

#b = main()    
