# -*- coding: utf-8 -*-
#Algorithm to extract parties from contracts
import nltk
from nltk.tag import stanford
from nltk.tag.stanford import POSTagger
import re
import os


#global variables
path = '/home/lsa/Dropbox/PUCRS/Dissertation/Corpus/xibinCorpus/noHTML/licence'
#path = 'data/contratos/between'
jar = 'stanford-postagger.jar'
model = 'wsj-0-18-bidirectional-distsim.tagger'
stanford = POSTagger(model, jar)
sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
output = open('data/contracts_and_their_entities.txt', 'w')
wrong = open('data/problem_files', 'w')

#function that finds entities in the sentence and returns a list with them
def find_entities(block_entities):
    pre_entities = []
    pos_entities = []
    nicknames    = []

    #grammar to extract named entities
    grammar = "Entity: {<DT>?(<NN>|<CC>|<NNP>|<IN>)*(<NNP>|<NNS>)}"
    finder  = nltk.RegexpParser(grammar)
    regex   = re.compile("``.+?''")
#    print block_entities
    if block_entities.__contains__('AND'):
        try:
            pre_entities.append(' '.join(block_entities[1:block_entities.index('AND')]))
            pre_entities.append(' '.join(block_entities[block_entities.index('AND')+1:]))
        except:
            pass
    else:
        try:
            pre_entities.append(' '.join(block_entities[1:block_entities.index('and')]))
            pre_entities.append(' '.join(block_entities[block_entities.index('and')+1:]))
        except:
            pass
    
    if len(pre_entities) > 1:
        output.write("Entity 1: " + str(pre_entities[0]) + "\n\nEntity 2: " + str(pre_entities[1]) + "\n\n")

"""    for entity in pre_entities:
        pos_entity = stanford.tag(nltk.word_tokenize(entity))
        tree = finder.parse(pos_entity)        
        #print tree
        try:
            if tree[0].label() == 'Entity':
                entity = [x for x, y in tree[0]]
                entity = ' '.join(entity)
                pos_entities.append(entity)
            else:
                pos_entities.append(entity)
        except:
            print "Problems\n"
        
#    print pos_entities    

#   End of the first part
    
    for entity in pre_entities:
        nickname = regex.findall(entity)
        if nickname:
            nicknames.append(nickname[0].split())
        else:
            nicknames.append("Entity without nickname")
"""
 
#    if len(nicknames) == len(pos_entities):
#        for index in range(len(nicknames)):
#                print "Entity: " + pos_entities[index] + "\nNickname: " + nicknames[index] + "\n"
            
#function that opens and process the contracts in 'contracts_path'
def extract_parties(*list_contracts):
    counter = 0
    if not list_contracts:
        list_contracts = os.listdir(path)

    regex = re.compile('BETWEEN[\r]*.+ AND.+', re.I)   
    for contract_file in list_contracts:
        flag = False
        output.write("=====================================================================\n")
        output.write("File name: " + contract_file + "\n\n")
        contract_text = ' '.join(open(path+"/"+contract_file, 'r').read().split())
        try:
            contract_sents = sent_tokenizer.tokenize(contract_text)
        except:
            print "I can't parse this file: ", contract_file
        for sentence in contract_sents:            
            list_sent = nltk.word_tokenize(sentence)
            sentence = ' '.join(list_sent)
            block = regex.findall(sentence)
            if block:
                flag = True
                counter += 1
#                print "Contract sentence: " + sentence +"\n\n"
                block_entities = block[0].split()
                entities = find_entities(block_entities)
                break
        if not flag:
            wrong.write(contract_file + "\n\n")
    
    print counter    
#calls function    
extract_parties()#'1043133.lic.1997.shtml')
